{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definition\n",
    "\n",
    "Given the dataset $(X, y)$ of size $n \\in \\mathbb{N}$ having $K \\in \\mathbb{N}$ variables, where $X \\in \\mathbb{R}^{n \\times K}$ and $y \\in \\mathbb{R}^n$. \n",
    "\n",
    "Suppose that there exists a function $f: \\mathbb{R}^{1 \\times K} \\rightarrow \\mathbb{R}, x_n \\rightarrow y_n$ ($x_n \\in \\mathbb{R}^{1 \\times K}$ and $y_n \\in \\mathbb{R}$). Regression tasks require us to approximate this function $f$ using the provided dataset.\n",
    "\n",
    "Suppose $y_n$ is linearly dependent on $x_n$. We therefore assume that $f$ is a linear function. Hence it will have a form of \n",
    "$$\n",
    "    y_n = f(x_n) =\\beta_0 + x_n \\beta \\in \\mathbb{R}\n",
    "$$\n",
    "where $\\beta \\in \\mathbb{R}^K$ and $\\beta_0 \\in \\mathbb{R}$.\n",
    "\n",
    "Our objective is to approximate $f$ so that $x_n \\beta$ is closed to $y_n$ as much as posible. In the other words, we are going to minize the Sum-of-Square Error Function:\n",
    "$$\n",
    "    E(X, y, \\beta) = \\frac{1}{2}\\sum_{i=1}^n{(y_n - f(x_n))^2} = \\frac{1}{2}\\sum_{i=1}^n{(y_n - x_n \\beta)^2} = \\frac{1}{2} ||y - \\beta X||^2\n",
    "$$\n",
    "The term $\\frac{1}{2}$ is prefixed for later convenience in derivative computation.\n",
    "\n",
    "The linear function $f$ is depent on the parameter $\\beta$. So we can reformulate the problem as:\n",
    "$$\n",
    "    \\beta* = argmin_{\\beta}(E(X, y, \\beta))\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ordinary Least Square Estimator\n",
    "Given a linear regression model under the following form:\n",
    "\n",
    "$$\n",
    "    y = X\\beta + \\varepsilon = \\sum_{i=1}^D \\beta_i x_i + \\varepsilon\n",
    "$$\n",
    "\n",
    "The estimator for this linear regression model having the form\n",
    "\n",
    "$$\n",
    "    \\hat{\\beta} = (X^TX)^{-1} X^Ty\n",
    "$$\n",
    "\n",
    "is called the **Ordinary Least Square** (OLS) estimator for the above linear regression model.\n",
    "\n",
    "According to Gauss-Markov theorem, OLS estimator has the lowest *sampling variance* (see below) among estimators in the class of *linear unbiased estimator* (see below) given the two following assumptions:\n",
    "- The errors in the linear regression model are uncorrelated.\n",
    "- The errors in the linear regression model all have zero expectation and equal variances.\n",
    "\n",
    "Formally, given a regression function $y_i = x_i\\beta + \\varepsilon_i$ where $(\\varepsilon_i)_i$ is the sequence of errores, we restate the two assumptions as:\n",
    "- $\\forall (i, j)$ that $i \\neq j$, $Cov(\\varepsilon_i, \\varepsilon_j) = 0$.\n",
    "- $\\forall i$, $\\mathbb{E}[\\varepsilon_i] = 0$ and $Var(\\varepsilon_i) = \\sigma^2 < +\\infty$. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bias of an Estimator\n",
    "\n",
    "the bias of an estimator is the difference between the expected value of the paremater of that estimator and the true value of the parameter being estimated. An estimator is called **unbiased** if its bias is zero. In other case, it is called **bias estimator**.\n",
    "\n",
    "Formally, suppose we have a statistical model $P_{\\theta}(x) = P(x | \\theta)$ parameterized by $\\theta$. Let $\\hat{\\theta}$ be the estimator of $\\theta$. Then the *bias* of $\\hat{\\theta}$ relative to $\\theta$ is defined as\n",
    "$$\n",
    "    Bias(\\hat{\\theta}, \\theta) = Bias_{}(\\hat{\\theta}, \\theta) = \\mathbb{E}_{x | \\theta} [\\hat{\\theta}] - \\theta = \\mathbb{E}_{x | \\theta} [\\hat{\\theta} - \\theta]\n",
    "$$\n",
    "where $\\mathbb{E}_{x | \\theta}$ denotes expected value over the distribution $P(x | \\theta)$: \n",
    "$$\n",
    "    \\mathbb{E}_{x | \\theta}[\\hat{\\theta}] = \\int \\hat{\\theta} P(x | \\theta) dX = \\int (x^Tx)^{-1}(x^Ty) P(x | \\theta) dx\n",
    "$$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Proof that OLS is a Linear Unbiased Estimator\n",
    "\n",
    "First, we have to show that $\\hat{\\beta} = (X^T X)^{-1} X^T y$ is an estimator of the given linear regression model.\n",
    "\n",
    "Indeed, we have\n",
    "\n",
    "\\begin{align}\n",
    "    E(X, y, \\beta)  & = ||y - X\\beta||^2 \\\\\n",
    "                    & = (y - X \\beta)^T (y - X \\beta) \\\\\n",
    "                    & = (y^T y - \\beta^T X^T y - y^T X\\beta + \\beta^T X^T X\\beta)\n",
    "\\end{align}\n",
    "\n",
    "Then\n",
    "\n",
    "\\begin{align}\n",
    "    \\frac{dE(X, y, \\beta)}{d\\beta}  & = \\frac{d}{d\\beta}(y^T y - \\beta^T X^T y - y^T X\\beta + \\beta^T X^T X\\beta) \\\\\n",
    "                                    & = -2X^T y + 2X^T X\\beta \\\\\n",
    "\\end{align}\n",
    "\n",
    "So\n",
    "\n",
    "\\begin{align}\n",
    "    & & \\frac{dE(X, y, \\beta)}{d\\beta} & = & 0 \\\\\n",
    "    & \\Leftrightarrow & -2X^T y + 2X^T X\\beta & = & 0 \\\\\n",
    "    & \\Leftrightarrow &  X^T X\\beta & = & X^T y \\\\\n",
    "    & \\Leftrightarrow &  \\beta & = & (X^T X)^{-1}X^T y \\\\\n",
    "\\end{align}\n",
    "with the assumption that $rank(X) = K$ so that $X^T X$ is invertible.\n",
    "\n",
    "To this end, we can see that\n",
    "\n",
    "\\begin{align}\n",
    "    \\mathbb{E}[\\hat{\\beta}] & = \\mathbb{E}[(X^T X)^{-1}X^T y] \\\\\n",
    "                            & = \\mathbb{E}[(X^T X)^{-1}X^T (X\\beta + \\varepsilon)] \\\\\n",
    "                            & = \\mathbb{E}[(X^T X)^{-1}X^T X\\beta + (X^T X)^{-1}X^T \\varepsilon] \\\\\n",
    "                            & = \\mathbb{E}[\\beta + (X^T X)^{-1}X^T \\varepsilon] \\\\\n",
    "                            & = \\beta + \\mathbb{E}[(X^T X)^{-1}X^T \\varepsilon] \\\\\n",
    "                            & = \\beta + (X^T X)^{-1}X^T \\mathbb{E}[\\varepsilon] \\\\\n",
    "                            & = \\beta \\\\\n",
    "\\end{align}\n",
    "hence $Bias(\\hat{\\theta}, \\theta) = 0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Proof that OLS is a Best Linear Unbiased Estimator (BLEU)\n",
    "\n",
    "To show that OLS estimator is the best linear unbiased estimator for the linear regression model, let $\\tilde{\\beta} = ((X^T X)^{-1} X^T + D)y$ is another linear estimation where $D \\in \\mathbb{R^{K \\times n}}$ is a non-zero matrix, we will show that such estimator can not have smaller variance than those of $\\beta$.\n",
    "\n",
    "We have:\n",
    "\n",
    "\\begin{align}\n",
    "    \\mathbb{E}[\\tilde{\\beta}]   & = \\mathbb{E}[((X^T X)^{-1} X^T + D)y] \\\\\n",
    "                                & = \\mathbb{E}[((X^T X)^{-1} X^T + D)(X \\beta + \\epsilon)] \\\\\n",
    "                                & = ((X^T X)^{-1} X^T + D)X \\beta + ((X^T X)^{-1} X^T + D)\\mathbb{E}[\\varepsilon] \\\\\n",
    "                                & = ((X^T X)^{-1} X^T + D)X \\beta & (\\mathbb{E}[\\varepsilon] = 0) \\\\\n",
    "                                & = (X^T X)^{-1} X^T X \\beta + D X \\beta \\\\\n",
    "                                & = (I_K + DX) \\beta\n",
    "\\end{align}\n",
    "\n",
    "As $\\hat{\\beta}$ is unbiased ($\\mathbb{E}[\\hat{\\beta}] = \\beta$), we must have $DX = 0$ or $D = 0$.\n",
    "\n",
    "Moreover, to see that $\\hat{\\beta}$ is an estimator that has the minimum variance, let:\n",
    "\n",
    "\\begin{align}\n",
    "    Var(l^T \\tilde{\\beta})  & = l^T Var(\\tilde{\\beta})l \\\\\n",
    "                            & = l^T (\\sigma^2 (X^T X)^{-1}) l + l^T DD^T l \\\\\n",
    "                            & = \\sigma^2 l^T (X^T X)^{-1} l + l^T DD^T l \\\\\n",
    "                            & = Var(l^T \\hat{\\beta}) + (D^T l)^T (D^T l) \\\\\n",
    "                            & = Var(l^T \\hat{\\beta}) + || D^T l || \\ge Var(l^t \\hat{\\beta})\n",
    "\\end{align}\n",
    "\n",
    "Equatily holds if and ony if (iif) $D^T l = 0$. In this case, we have:\n",
    "\n",
    "\\begin{align}\n",
    "    l^T \\tilde{\\beta}  & = l^T (((X^T X)^{-1} X^T + D)y) \\\\\n",
    "                            & = l^T (X^T X)^{-1} X^T y + l^T Dy \\\\\n",
    "                            & = l^T \\hat{\\beta} + (D^T l)^T y \\\\\n",
    "                            & = l^T \\hat{\\beta}\n",
    "\\end{align}\n",
    "\n",
    "This proves the uniqueless of the OLS estimator as the BLEU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Overfitting and Regularization\n",
    "\n",
    "![image.png](attachment:image.png)\n",
    "\n",
    "When there is a small number of data points ($n$) but we have a large amount of variables ($K$) (that is $n << K$) then the overfitting of regression models has high change to occur. There are two ways to tackle this problem:\n",
    "- Collecting more data: more data results in more constraints in the error function $E(X, y, \\beta)$ hence limit the change of overfitting (see the above figure).\n",
    "- Adding the regularization terms (usually the regular term $\\beta^T\\beta$) in the objective function.\n",
    "\n",
    "The following sections will discuss the second case where the motivations of why regularization terms are drawn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Regression model from the Maximum Likelihood (ML) approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let assume that the noise introduced in the regression model (generally) $\\varepsilon \\sim \\mathcal{N}(0, \\theta^{-1})$. As we have\n",
    "\n",
    "$$\n",
    "    y = f(x, \\beta) + \\varepsilon\n",
    "$$\n",
    "\n",
    "given $x \\in \\mathbb{R}^K$ and $y \\in \\mathbb{R}$ , so we have\n",
    "\n",
    "$$\n",
    "    P(y | x, \\beta, \\theta) \\sim \\mathcal{N}(y | f(x, \\beta), \\theta^{-1})\n",
    "$$\n",
    "\n",
    "where $\\beta$ is the parameters of the function $f$.\n",
    "\n",
    "Given the dataset $(X, T)$ where $X = (x_1, x_2, ..., x_n)^T \\in \\mathbb{R}^{n \\times K}$ and $T = (t_1, t_2, ..., t_n)^T \\in \\mathbb{R}^n$, we have:\n",
    "\n",
    "$$\n",
    "    P(T | X, \\theta, \\beta) = \\prod_{i=1}^{n} P(t_i | f(x_i, \\beta)) = \\prod_{i=1}^{n} \\mathcal{N}(t_i | f(x_i, \\beta), \\theta^{-1})\n",
    "$$\n",
    "\n",
    "For the ease of later calculation, let turn these formulas into logarithmic space:\n",
    "\n",
    "\\begin{align}\n",
    "    ln P(T | X, \\theta, \\beta)  & = \\sum_{i=1}^{n} ln \\mathcal{N}(t_i | f(x_i, \\beta), \\theta^{-1}) \\\\\n",
    "                                & = -\\frac{\\theta}{2} \\sum_{i=1}^{n} (t_n - f(x_n, \\beta))^2 + \\frac{n}{2} ln\\theta - \\frac{N}{2}ln 2\\pi \\\\\n",
    "                                & = -\\theta E(X, T, \\beta) + const\n",
    "\\end{align}\n",
    "\n",
    "Our target is to find the set of $(\\beta, \\theta)$ so that the likelihood $P(T | X, \\theta, \\beta)$ reaches its maximum capility on the dataset $(X, T)$. As the $ln$ is a monotonic increasing function on $\\mathbb{R}$, **maximizing that likelihood is equavalent to minimizing the log likelihood**. Moreover, **finding the optimized parameter $\\beta$ of the regression model (in general) using the Sum-of-Square Error Function implies maximizing the likelihood with assumption of Gaussian distribution of the noise with fixed $\\theta$ parameter**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Regression model from the Maximum A Posterior (MAP) approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Maximum Likelihood approach assumes the $\\theta$ parameter of noise is fixed to determine the optimized parameter of the regression model. Such approach is called the **point estimation**. To comprehensively describe the variability of the parameter $\\beta$ of regression model, we apply the Bayesian rule to have\n",
    "\n",
    "$$\n",
    "    P(\\beta | X, T, \\alpha, \\theta) \\propto P(T | X, \\theta, \\beta) P(\\beta | \\alpha)\n",
    "$$\n",
    "where $P(\\beta | \\alpha)$ is the prior distribution of the $\\beta$ parameter, $P(\\beta | X, T, \\alpha, \\theta)$ is the posterior distribution of $\\theta$ parameter, and $P(T | X, \\theta, \\beta)$ is the likelihood distribution of the $\\theta$ parameter.\n",
    "\n",
    "To this end, let\n",
    "$$\n",
    "    P(\\beta | \\alpha) = \\mathcal{N} (\\beta | 0, \\alpha^{-1}I_K) = \\left(\\frac{\\alpha}{2\\pi}\\right)^{\\frac{K+1}{2}} exp\\left(-\\frac{\\alpha}{2} \\beta^T \\beta\\right)\n",
    "$$\n",
    "where $\\alpha$ is the hyperparameter, and\n",
    "$$\n",
    "    P(T | X, \\theta, \\beta) = \\mathcal{N}(T | f(X, \\beta), \\beta^{-1})\n",
    "$$\n",
    "\n",
    "Let turn these probabilities into the logarithmic space for later convenience in computation, we have:\n",
    "\n",
    "$$\n",
    "    ln P(\\beta | \\alpha) = \\frac{K+1}{2}ln\\frac{\\alpha}{2\\pi} - \\frac{\\alpha}{2} \\beta^T\\beta\n",
    "$$\n",
    "\n",
    "and\n",
    "\n",
    "\\begin{align}\n",
    "    ln P(T | X, \\theta, \\beta)  & = \\sum_{i=1}^{n} ln \\mathcal{N}(t_i | f(x_i, \\beta), \\theta^{-1}) \\\\\n",
    "                                & = -\\frac{\\theta}{2} \\sum_{i=1}^{n} (t_n - f(x_n, \\beta))^2 + \\frac{n}{2} ln\\theta - \\frac{N}{2}ln 2\\pi\n",
    "\\end{align}\n",
    "\n",
    "Hence we have:\n",
    "\n",
    "\\begin{align}\n",
    "    ln P(\\beta | X, T, \\alpha, \\theta)  & = \\frac{K+1}{2}ln\\frac{\\alpha}{2\\pi} - \\frac{\\alpha}{2} \\beta^T\\beta - \\frac{\\theta}{2} \\sum_{i=1}^{n} (t_n - f(x_n, \\beta))^2 + \\frac{n}{2} ln\\theta - \\frac{N}{2}ln 2\\pi \\\\\n",
    "                                        & = - \\frac{\\theta}{2} \\sum_{i=1}^{n} (t_n - f(x_n, \\beta))^2 - \\frac{\\alpha}{2} \\beta^T\\beta  + const \\\\\n",
    "                                        & = -\\theta E(X, T, \\beta) - \\frac{\\alpha}{2} \\beta^T\\beta + const\n",
    "\\end{align}\n",
    "where $const$ indicates terms that are not relevant to the $\\beta$ parameter.\n",
    "\n",
    "**This implies that minimizing objective function $E(X, T, \\beta)$ using the regularization term $\\beta^T\\beta$ is equavalent to Maximizing a Posterion with the assumption of Gaussian distribution of the likelihood and prior distribution.**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
