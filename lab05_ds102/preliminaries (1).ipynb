{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70066580",
   "metadata": {},
   "source": [
    "## K-means Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe1919f8",
   "metadata": {},
   "source": [
    "Suppose we have a dataset $D = \\{x_1, x_2, ..., x_N\\}$ consistings of $N \\in \\mathbb{N}^*$ observations of a random D-dimensional Euclidean variable $X \\in \\mathbb{R}^D$.\n",
    "\n",
    "\n",
    "We aim to explore the internal relation among data points through observations. That is, we want to find some groups of data points, each group represent a particular attributes of the dataset.\n",
    "\n",
    "We start the mentioned idea by constructing some notations:\n",
    "\n",
    "- Assume our dataset $D$ has $K \\in \\mathbb{N}^*$ groups. Let $(\\mu_i)_{1 \\le i \\lq K}$ the sequence of value represented for respective clusters. Therefore $\\mu_i \\in \\mathbb{R}^D$ for any $1 \\le i \\le K$.\n",
    "- Let's define the indicator \n",
    "$$\n",
    "r_{nk} =\n",
    "\\begin{cases}\n",
    "    0, & \\text{if $x_n$ is assigned to the cluster $k^{th}$}\\\\\n",
    "    1, & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "- Our goal is to determine the clusters for each data point $x_i \\in D$ as well as the value of $\\mu_i$ in order that we have the following function approach its mimimum:\n",
    "$$\n",
    "    J = \\sum_{n=1}^N{\\sum_{k=1}^K{r_{nk}||x_n - \\mu_k||^2}}\n",
    "$$\n",
    "\n",
    "We can perform the optimization algorithm as the follows:\n",
    "\n",
    "- First, we randomly select values for $(\\mu_i)_{1 \\le i \\le K}$. Then we minimize $J$ with respect to (w.r.t.) $r_{nk}$ while keeping $\\mu_k$ fixed.\n",
    "- Next, we keep the $r_{nk}$ fixed and minize $J$ w.r.t. $\\mu_k$\n",
    "\n",
    "These two stages are repeatedly performed until the convergence.\n",
    "\n",
    "The described algorithm can be deployed by the Expectation - Maximization algorithm (EM).\n",
    "\n",
    "\n",
    "In the first stage, after we randomly initialized $\\mu_k$ and fixed them, we find the optimizal solution of $J$ w.r.t. $r_{nk}$. Formally, this is expressed as:\n",
    "$$\n",
    "    r_{nk} =\n",
    "    \\begin{cases}\n",
    "        1 & \\text{if $k = argmin_{i}{||x_n - \\mu_i||^2}$} \\\\\n",
    "        0 & \\text{otherwise}\n",
    "    \\end{cases}\n",
    "$$\n",
    "\n",
    "In the second stage, consider the fixed $r_{nk}$, we optimize $J$ w.r.t. $\\mu_k$ by letting their derivative w.r.t. $\\mu_k$ to $0$:\n",
    "$$\n",
    "    2\\sum_{n=1}^N{r_{nk}(x_n - \\mu_k)} = 0 \\Leftrightarrow \\sum_{n=1}^N{r_{nk}x_n} = \\mu_k \\sum_{n=1}^N{r_{nk}} \\Leftrightarrow \\mu_k = \\frac{\\sum_{n=1}^N{r_{nk}x_n}}{\\sum_{n=1}^N{r_{nk}}}\n",
    "$$\n",
    "\n",
    "The denominator of this expression is the number of data points assigned to cluster $k^{th}$ $\\Rightarrow$ $\\mu_k$ is simply interpreted as the mean of $x_n$ assigned to cluster $k^{th}$, whichs leads to the name K-means Clustering of EM for optimizing $J$.\n",
    "\n",
    "The K-means algorithm is repeatedly performed until there is no update for $(\\mu_k)$ or the total number of interations exceed a number."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d642f679",
   "metadata": {},
   "source": [
    "## Gaussian Mixtures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2451320",
   "metadata": {},
   "source": [
    "Previous methods mainly inspired by a single Gaussian distribution which is not comprehensive to describe the characteristics of real data. For instance, let consider the following example where the distribution of data is given by the combination of multiple Gaussian distributions (the read line is the data distribution while the blue ones are single Gaussian distribution).\n",
    "\n",
    "Instead of having one distribution to describe data, we can define a linear combination of multiple distributions to better describe their complexity. Such distribution is called **mixture distributions**. In the case we are working with Gaussian distribution, this distribution is called **Gaussian Mixture**. Their formal form is written as:\n",
    "\n",
    "\\begin{equation}\n",
    "    p(x) = \\sum_{k=1}^K \\pi_k \\mathcal{N}(x | \\mu_k , \\Sigma_k)\n",
    "\\end{equation}\n",
    "Each Gaussian density is called a component of that Gaussian Mixture method with its own expectation $\\mu_k$ and covariance $\\Sigma_k$.\n",
    "\n",
    "If we integrate (1) both side with respect to (w.r.t.) $x$, then we obtain:\n",
    "\n",
    "\\begin{align}\n",
    "    &                   & \\int p(x)dx = \\int \\pi_k \\mathcal{N}(x | \\mu_k , \\Sigma_k)dx \\\\\n",
    "    & \\Leftrightarrow   & \\sum_{k=1}^K \\pi_k \\int \\mathcal{N}(x | \\mu_k, \\Sigma_k)dx = 1 \\\\\n",
    "    & \\Leftrightarrow   & \\sum_{k=1}^K \\pi_k = 1\n",
    "\\end{align}\n",
    "\n",
    "Note that we have $p(x) \\le 0$ $\\forall x$, together with $\\mathcal{N}(x|\\mu_k, \\Sigma_k) \\le 0$ $\\forall x, k$, which implies $\\pi_k \\le 0$ $\\forall k$. To this end, we have:\n",
    "\n",
    "$$\n",
    "    0 \\le \\pi_k \\le 1\n",
    "$$\n",
    "\n",
    "If we define $\\pi_k = p(k)$ as the prior probability of picking the $k^{th}$ component, and $p(x | k) = \\mathcal{N}(x | \\mu_k, \\Sigma_k)$ as the probability of having $x$ given $k$, we obtain:\n",
    "\n",
    "$$\n",
    "    p(x) = \\sum_{k=1}^K p(x, k) = \\sum_{k=1}^K p(x|k)p(k) = \\sum_{k=1}^K \\pi_k \\mathcal{N}(x | \\mu_k, \\Sigma_k)\n",
    "$$\n",
    "we again return to the formula (1). In this form, $k$ is the **latent variable**, which is recognized as one of the **latent factors** that affects how $x$ occurs.\n",
    "\n",
    "A common way of approximating the parameters of these $k$ Gaussian distributions is deploying maximum likelihood method. From the above formula, the likelihood probability is given by\n",
    "\n",
    "$$\n",
    "    p(k | x) = \\frac{p(k)p(x|k)}{p(x)} = \\frac{p(k)p(x|k)}{\\sum_{m=1}^K p(x, m)} = \\frac{p(k)p(x|k)}{\\sum_{m=1}^K p(m)p(x|m)} = \\frac{\\pi_k \\mathcal{N}(x | \\mu_k, \\Sigma_k)}{\\sum_{m=1}^K \\pi_m \\mathcal{N}(\\mu_m, \\Sigma_m)}\n",
    "$$\n",
    "\n",
    "Therefore, we obtain the likelihood function as follows:\n",
    "\n",
    "$$\n",
    "    ln p(X|\\pi, \\mu, \\Sigma) = \\sum_{n=1}^N ln \\left[ \\sum_{k=1}^K \\pi_k \\mathcal{N}(x_n | \\mu_n, \\Sigma_n) \\right]\n",
    "$$\n",
    "where $X = \\{x_1, x_2, ..., x_N\\}$, $\\pi = \\{\\pi_1, \\pi_2, ..., \\pi_K\\}$, $\\mu = \\{\\mu_1, \\mu_2, \\mu_3, ..., \\mu_K\\}$, and $\\Sigma = \\{ \\Sigma_1, \\Sigma_2, ..., \\Sigma_K \\}$\n",
    "\n",
    "However, the maximum likelihood solution for the above likelihood function is not close-form anymore. Former studies tried to applied numerical optimization techniques, but we can deploy the more powerful **Expectation-Maximization** framework instead.\n",
    "\n",
    "Before going to the EM framework for Gaussian Mixtures, let determine the formula of each $\\mu_k$. From the likelihood function given above, we have:\n",
    "\n",
    "\\begin{align}\n",
    "    \\frac{\\delta}{\\delta \\mu_k} ln p(X | \\pi, \\mu, \\Sigma)  & = \\sum_{n=1}^N \\frac{\\delta}{\\delta \\mu_k} ln \\left[ \\sum_{m=1}^K \\pi_m \\mathcal{N}(x_n | \\mu_m, \\Sigma_m) \\right] \\\\\n",
    "                                                            & = \\sum_{n=1}^N \\left(\\sum_{m=1}^K \\pi_m \\mathcal{N}(x_n | \\mu_m, \\Sigma_m)\\right)^{-1} \\frac{\\delta}{\\delta \\mu_k} \\sum_{m=1}^K \\pi_m \\mathcal{N}(x_n | \\mu_m, \\Sigma_m) \\\\\n",
    "                                                            & = \\sum_{n=1}^N \\left(\\sum_{m=1}^K \\pi_m \\mathcal{N}(x_n | \\mu_m, \\Sigma_m)\\right)^{-1} \\sum_{m=1}^K \\frac{\\delta}{\\delta \\mu_k} \\pi_m \\mathcal{N}(x_n | \\mu_m, \\Sigma_m) \\\\\n",
    "                                                            & = \\sum_{n=1}^N \\left(\\sum_{m=1}^K \\pi_m \\mathcal{N}(x_n | \\mu_m, \\Sigma_m)\\right)^{-1} \\sum_{m=1}^K \\frac{\\delta}{\\delta \\mu_k} \\pi_m \\frac{1}{(2\\pi)^\\frac{D}{2}} \\frac{1}{|\\Sigma_m|^2} exp\\left(-\\frac{1}{2}(x_n - \\mu_m)^T \\Sigma_m^{-1} (x_n - \\mu_m)\\right) \\\\\n",
    "                                                            & = \\sum_{n=1}^N \\left(\\sum_{m=1}^K \\pi_m \\mathcal{N}(x_n | \\mu_m, \\Sigma_m)\\right)^{-1} \\sum_{m=1}^K \\pi_m \\frac{1}{(2\\pi)^\\frac{D}{2}} \\frac{1}{|\\Sigma_m|^2} \\frac{\\delta}{\\delta \\mu_k} exp\\left(-\\frac{1}{2}(x_n - \\mu_m)^T \\Sigma_m^{-1} (x_n - \\mu_m)\\right) \\\\\n",
    "                                                            & = \\sum_{n=1}^N \\left(\\sum_{m=1}^K \\pi_m \\mathcal{N}(x_n | \\mu_m, \\Sigma_m)\\right)^{-1} \\sum_{m=1}^K \\pi_m \\frac{1}{(2\\pi)^\\frac{D}{2}} \\frac{1}{|\\Sigma_m|^2} exp\\left(-\\frac{1}{2}(x_n - \\mu_m)^T \\Sigma_m^{-1} (x_n - \\mu_m)\\right) \\frac{\\delta}{\\delta \\mu_k} \\left( -\\frac{1}{2}(x_n - \\mu_m)^T \\Sigma_m^{-1} (x_n - \\mu_m)\\right) \\\\\n",
    "                                                            & = \\sum_{n=1}^N \\left(\\sum_{m=1}^K \\pi_m \\mathcal{N}(x_n | \\mu_m, \\Sigma_m)\\right)^{-1} \\sum_{m=1}^K \\pi_m \\frac{1}{(2\\pi)^\\frac{D}{2}} \\frac{1}{|\\Sigma_m|^2} exp\\left(-\\frac{1}{2}(x_n - \\mu_m)^T \\Sigma_m^{-1} (x_n - \\mu_m)\\right) \\frac{\\delta}{\\delta \\mu_k} \\left( -\\frac{1}{2} x_n^T \\Sigma_m^{-1} x_n + \\mu_m^T\\Sigma_m^{-1} x_n - \\frac{1}{2} \\mu_m^T \\Sigma_m^{-1} \\mu_m  \\right) \\\\\n",
    "                                                            & = \\sum_{n=1}^N \\frac{\\pi_k \\frac{1}{(2\\pi)^\\frac{D}{2}} \\frac{1}{|\\Sigma_k|^2} exp\\left(-\\frac{1}{2}(x_n - \\mu_k)^T \\Sigma_k^{-1}(x_n - \\mu_k)\\right)}{\\sum_{m=1}^K \\pi_m \\mathcal{N}(x_n | \\mu_m, \\Sigma_m)} \\Sigma_k^{-1} \\left(  x_n - \\mu_k  \\right) \\\\\n",
    "                                                            & = \\sum_{n=1}^N \\frac{\\pi_k \\mathcal{N}(x_n | \\mu_k, \\Sigma_k)}{\\sum_{m=1}^K \\pi_m \\mathcal{N}(x_n | \\mu_m, \\Sigma_m)} \\Sigma_k^{-1} \\left(  x_n - \\mu_k  \\right) \\\\\n",
    "                                                            & = \\sum_{n=1}^N p(k | x_n) \\Sigma_k^{-1} \\left(  x_n - \\mu_k  \\right) \\\\\n",
    "\\end{align}\n",
    "\n",
    "Taking this partial derivate to $0$ we obtain:\n",
    "\n",
    "\\begin{align}\n",
    "    & &                 & \\frac{\\delta}{\\delta \\mu_k} = 0 \\\\\n",
    "    & & \\Leftrightarrow & \\sum_{n=1}^N p(k | x_n) \\Sigma_k^{-1} \\left(  x_n - \\mu_k  \\right) = 0 \\\\\n",
    "    & & \\Leftrightarrow & \\sum_{n=1}^N p(k | x_n) \\mu_k = \\sum_{n=1}^N p(k | x_n) x_n \\\\\n",
    "    & & \\Leftrightarrow & \\mu_k = \\frac{\\sum_{n=1}^N p(k | x_n) x_n}{\\sum_{n=1}^N p(k | x_n) } = \\frac{1}{N_k} \\sum_{n=1}^N p(k | x_n) x_n \\\\\n",
    "\\end{align}\n",
    "where $N_k = \\sum_{n=1}^N p(k | x_n)$. In this formula, $p(k | x_n)$ can be interpreted as the effective number of how much factor $k$ contributes to the occurance of $x_n$. For further ease of notation, let define $\\gamma_k(x_n) \\equiv p(k | x_n)$. To this end, the quanlity $N_k$ is interpreted as how many $x_n$ influenced by the factor $k$.\n",
    "\n",
    "Applying the same technique for partial derivative of the likelihood function w.r.t $\\Sigma_k$, we have:\n",
    "\n",
    "\\begin{align}\n",
    "    \\frac{\\delta}{\\delta \\Sigma_k} ln p(X | \\pi, \\mu, \\Sigma)  & = \\sum_{n=1}^N \\frac{\\delta}{\\delta \\Sigma_k} ln \\left[ \\sum_{m=1}^K \\pi_m \\mathcal{N}(x_n | \\mu_m, \\Sigma_m) \\right] \\\\\n",
    "                                                            & = \\sum_{n=1}^N \\left(\\sum_{m=1}^K \\pi_m \\mathcal{N}(x_n | \\mu_m, \\Sigma_m)\\right)^{-1} \\frac{\\delta}{\\delta \\Sigma_k} \\sum_{m=1}^K \\pi_m \\mathcal{N}(x_n | \\mu_m, \\Sigma_m) \\\\\n",
    "                                                            & = \\sum_{n=1}^N \\left(\\sum_{m=1}^K \\pi_m \\mathcal{N}(x_n | \\mu_m, \\Sigma_m)\\right)^{-1} \\sum_{m=1}^K \\frac{\\delta}{\\delta \\Sigma_k} \\pi_m \\mathcal{N}(x_n | \\mu_m, \\Sigma_m) \\\\\n",
    "                                                            & = \\sum_{n=1}^N \\left(\\sum_{m=1}^K \\pi_m \\mathcal{N}(x_n | \\mu_m, \\Sigma_m)\\right)^{-1} \\pi_k  \\frac{\\delta}{\\delta \\Sigma_k} \\mathcal{N}(x_n | \\mu_k, \\Sigma_k) \\\\\n",
    "\\end{align}\n",
    "\n",
    "Note that\n",
    "\n",
    "$$\n",
    "    \\frac{\\delta ln(f(x))}{\\delta x} = \\frac{1}{f(x)} \\frac{\\delta f(x)}{\\delta x} \\Rightarrow \\frac{\\delta f(x)}{\\delta x} = f(x) \\frac{\\delta ln(f(x))}{\\delta x}\n",
    "$$\n",
    "\n",
    "Apply this analysis to $\\frac{\\delta}{\\delta \\Sigma_k} \\mathcal{N}(x_n | \\mu_k, \\Sigma_k)$, we have:\n",
    "\n",
    "\\begin{align}\n",
    "    \\frac{\\delta}{\\delta \\Sigma_k} \\mathcal{N}(x_n | \\mu_k, \\Sigma_k) & = \\mathcal{N}(x_n | \\mu_k, \\Sigma_k) \\frac{\\delta}{\\delta \\Sigma_k} ln(\\mathcal{N}(x_n | \\mu_k, \\Sigma_k)) \\\\\n",
    "    & = \\mathcal{N}(x_n | \\mu_k, \\Sigma_k) \\frac{\\delta}{\\delta \\Sigma_k} ln \\left( \\frac{1}{(2\\pi)^{\\frac{D}{2}}} \\frac{1}{|\\Sigma_k|^\\frac{1}{2}} exp\\left( -\\frac{1}{2} (x_n - \\mu_k)^T \\Sigma_k (x_n - \\mu_k) \\right) \\right) \\\\\n",
    "    & = \\mathcal{N}(x_n | \\mu_k, \\Sigma_k) \\frac{\\delta}{\\delta \\Sigma_k} \\left( -\\frac{D}{2}ln(2\\pi) -\\frac{1}{2} ln|\\Sigma_k| -\\frac{1}{2} (x_n - \\mu_k)^T \\Sigma_k^{-1} (x_n - \\mu_k) \\right) \\\\\n",
    "    & = \\mathcal{N}(x_n | \\mu_k, \\Sigma_k) \\left( -\\frac{1}{2} \\Sigma_k^{-1} -\\frac{1}{2} \\Sigma_k^{-1} (x_n - \\mu_k)(x_n - \\mu_k)^T\\Sigma_k^{-1} \\right) \\\\\n",
    "\\end{align}\n",
    "\n",
    "So\n",
    "\n",
    "\\begin{align}\n",
    "    \\frac{\\delta}{\\delta \\Sigma_k} ln p(X | \\pi, \\mu, \\Sigma)   & = \\sum_{n=1}^N \\left(\\sum_{m=1}^K \\pi_m \\mathcal{N}(x_n | \\mu_m, \\Sigma_m)\\right)^{-1} \\pi_k  \\frac{\\delta}{\\delta \\Sigma_k} \\mathcal{N}(x_n | \\mu_k, \\Sigma_k) \\\\\n",
    "                                                                & = \\sum_{n=1}^N \\left(\\sum_{m=1}^K \\pi_m \\mathcal{N}(x_n | \\mu_m, \\Sigma_m)\\right)^{-1} \\pi_k \\mathcal{N}(x_n | \\mu_k, \\Sigma_k) \\left( -\\frac{1}{2} \\Sigma_k^{-1} -\\frac{1}{2} \\Sigma_k^{-1} (x_n - \\mu_k)(x_n - \\mu_k)^T\\Sigma_k^{-1} \\right) \\\\\n",
    "                                                                & = \\sum_{n=1}^N \\frac{\\pi_k \\mathcal{N}(x_n | \\mu_k, \\Sigma_k)}{\\sum_{m=1}^K \\pi_m \\mathcal{N}(x_n | \\mu_m, \\Sigma_m)} \\left( -\\frac{1}{2} \\Sigma_k^{-1} -\\frac{1}{2} \\Sigma_k^{-1} (x_n - \\mu_k)(x_n - \\mu_k)^T\\Sigma_k^{-1} \\right) \\\\\n",
    "                                                                & \\sum_{n=1}^N \\gamma_k(x_n) \\left( -\\frac{1}{2} \\Sigma_k^{-1} -\\frac{1}{2} \\Sigma_k^{-1} (x_n - \\mu_k)(x_n - \\mu_k)^T\\Sigma_k^{-1} \\right) \\\\\n",
    "                                                                \n",
    "\\end{align}\n",
    "\n",
    "Again, taking this partial derivate to $0$ we obtain:\n",
    "\n",
    "\\begin{align}\n",
    "    & &                 & \\frac{\\delta}{\\delta \\Sigma_k} =  0 \\\\\n",
    "    & & \\Leftrightarrow & \\sum_{n=1}^N \\gamma_k(x_n) \\left( -\\frac{1}{2} \\Sigma_k^{-1} -\\frac{1}{2} \\Sigma_k^{-1} (x_n - \\mu_k)(x_n - \\mu_k)^T\\Sigma_k^{-1} \\right) = 0 \\\\\n",
    "    & & \\Leftrightarrow & \\sum_{n=1}^N \\gamma_k(x_n) \\left[ 1 + (x_n - \\mu_k)(x_n - \\mu_k)^T\\Sigma_k^{-1} \\right] = 0 \\\\\n",
    "    & & \\Leftrightarrow & \\sum_{n=1}^N \\gamma_k(x_n) (x_n - \\mu_k)(x_n - \\mu_k)^T\\Sigma_k^{-1} = - \\sum_{n=1}^N \\gamma_k(x_n) \\\\\n",
    "    & & \\Leftrightarrow & \\Sigma_k = - \\frac{1}{N_k} \\sum_{n=1}^N \\gamma_k(x_n) (x_n - \\mu_k)(x_n - \\mu_k)^T \\\\\n",
    "\\end{align}\n",
    "\n",
    "Finally, we consider the formula of $\\pi_k$. Recall that this mixing coefficient is rules by the constraint $0 \\le \\sum_{k=1}^K \\pi_k \\le 1$. To take into account this constraint as well as maximizing the likelihood function $ln p(X | \\mu, \\pi, \\Sigma)$, we deploy the Lagrange multiplier as follows:\n",
    "\n",
    "$$\n",
    "    J = ln p(X | \\mu, \\pi, \\Sigma) + \\lambda \\left( \\sum_{k=1}^K \\pi_k - 1 \\right)\n",
    "$$\n",
    "\n",
    "Then the derivative of $J$ w.r.t $\\pi_k$ is determined by:\n",
    "\n",
    "\\begin{align}\n",
    "    \\frac{\\delta}{\\delta \\pi_k} \\left( ln p(X | \\mu, \\pi, \\Sigma) + \\lambda \\left( \\sum_{k=1}^K \\pi_k - 1 \\right) \\right) \n",
    "        & = \\sum_{n=1}^N \\frac{\\delta}{\\delta \\pi_k} ln \\left[ \\sum_{m=1}^K \\pi_m \\mathcal{N}(x_n | \\mu_m, \\Sigma_m) \\right] + \\lambda \\\\\n",
    "        & = \\sum_{n=1}^N \\frac{ \\frac{\\delta}{\\delta \\pi_k} \\sum_{m=1}^K \\pi_m \\mathcal{N}(x_n | \\mu_m, \\Sigma_m) }{\\left( \\sum_{m=1}^K \\pi_m \\mathcal{N}(x_n | \\mu_m, \\Sigma_m) \\right)}  + \\lambda \\\\\n",
    "        & = \\sum_{n=1}^N \\frac{ \\mathcal{N}(x_n | \\mu_k, \\Sigma_k) }{\\left( \\sum_{m=1}^K \\pi_m \\mathcal{N}(x_n | \\mu_m, \\Sigma_m) \\right)}  + \\lambda \\\\\n",
    "\\end{align}\n",
    "\n",
    "Then if we set this partial derivative to $0$ we get:\n",
    "\n",
    "$$\n",
    "    \\frac{\\delta J}{\\delta \\pi_k} = 0 \\Leftrightarrow \\sum_{n=1}^N \\frac{ \\mathcal{N}(x_n | \\mu_k, \\Sigma_k) }{ \\sum_{m=1}^K \\pi_m \\mathcal{N}(x_n | \\mu_m, \\Sigma_m) }  + \\lambda = 0 \\Leftrightarrow \\sum_{n=1}^N \\frac{ \\pi_k \\mathcal{N}(x_n | \\mu_k, \\Sigma_k) }{ \\sum_{m=1}^K \\pi_m \\mathcal{N}(x_n | \\mu_m, \\Sigma_m) } = - \\pi_k \\lambda \\Leftrightarrow \\sum_{n=1}^N \\gamma_k(x_n) = -\\pi_k \\lambda \\Leftrightarrow \\pi_k = -\\frac{1}{\\lambda} \\sum_{n=1}^N \\gamma_k(x_n)\n",
    "$$\n",
    "\n",
    "Taking into account the truth that $\\sum_{k=1}^K \\pi_k = 1$ we have:\n",
    "\n",
    "$$\n",
    "    \\sum_{k=1}^K \\pi_k = -\\frac{1}{\\lambda} \\sum_{k=1}^K \\sum_{n=1}^N \\gamma_k(x_n) = -\\frac{1}{\\lambda} \\sum_{k=1}^K N_k = \\frac{1}{\\lambda} N \\Rightarrow \\lambda = -N\n",
    "$$\n",
    "To this end, we have\n",
    "\n",
    "$$\n",
    "    \\pi_k = \\frac{N_k}{N}\n",
    "$$\n",
    "\n",
    "By determining the particular form of the variables, we can deploy the **Expectation-Maximization** (EM) framework to iteratively find the approximate parameters for these variables. The detailed EM method for training the Gaussian Mixtures is given as follows:\n",
    "\n",
    "1. Initialize the expectations $\\mu_k$, the covariances $\\Sigma_k$, and the mixing coefficients $\\pi_k$ randomly. Then evaluate the initial value of the log likelihood function.\n",
    "2. **E step:** Evaluate $\\gamma_k(x_n)$ using the current parameters\n",
    "\n",
    "$$\n",
    "    \\gamma_k(x_n) = \\frac{\\pi_k \\mathcal{N}(x_n | \\mu_k, \\Sigma_k)}{\\sum_{m=1}^K \\pi_m \\mathcal{N}(x_n | \\mu_m, \\Sigma_m)}\n",
    "$$\n",
    "\n",
    "3. **M step:** Re-estimate the parameters using the current $\\gamma_k(x_n)$\n",
    "\n",
    "$$\n",
    "    \\mu_k^{new} = \\frac{1}{N_k} \\sum_{n=1}^N \\gamma_k(x_n) x_n\n",
    "$$\n",
    "\n",
    "$$\n",
    "    \\Sigma_k^{new} = \\frac{1}{N_k} \\sum_{n=1}^N \\gamma_k(x_n) (x_n - \\mu_k^{new})(x_n - \\mu_k^{new})^T\n",
    "$$\n",
    "\n",
    "$$\n",
    "    \\pi_k^{new} = \\frac{N_k}{N}\n",
    "$$\n",
    "\n",
    "4. Evaluate the log likelohood\n",
    "\n",
    "$$\n",
    "    ln p(X| \\mu, \\pi, \\Sigma) = \\sum_{n=1}^N ln \\left[ \\sum_{k=1}^K \\pi_k \\mathcal{N}(x_n | \\mu_k, \\Sigma_k) \\right]\n",
    "$$\n",
    "then turn to the **E step**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d08cffed",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
